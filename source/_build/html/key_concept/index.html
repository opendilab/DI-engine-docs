

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Key Concept &mdash; DI-engine 0.1.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="强化学习基础概念介绍" href="../intro_rl/index_zh.html" />
    <link rel="prev" title="快速上手" href="../quick_start/index_zh.html" />
    <link href="../_static/css/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> DI-engine
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">使用者指南</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../installation/index_zh.html">安装说明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/index_zh.html">快速上手</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Key Concept</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#concept">Concept</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#environment">Environment</a></li>
<li class="toctree-l3"><a class="reference internal" href="#policy">Policy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#the-multi-mode-of-policy">The Multi-Mode of Policy</a></li>
<li class="toctree-l4"><a class="reference internal" href="#shared-model-model-wrapper">Shared Model + Model Wrapper</a></li>
<li class="toctree-l4"><a class="reference internal" href="#processing-function">Processing Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="#scale-up-to-parallel-training">Scale Up to Parallel Training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#config">Config</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id1">key concept</a></li>
<li class="toctree-l4"><a class="reference internal" href="#config-overview">config overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="#how-to-customize">How to customize?</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#worker-collector">Worker-Collector</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#serial-collector">Serial Collector</a></li>
<li class="toctree-l4"><a class="reference internal" href="#parallel-collector">Parallel Collector</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#worker-buffer">Worker-Buffer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#worker-evaluator">Worker-Evaluator</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#serial-evaluator">Serial Evaluator</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#worker-learner">Worker-Learner</a></li>
<li class="toctree-l3"><a class="reference internal" href="#entry">Entry</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#serial-pipeline">Serial Pipeline</a></li>
<li class="toctree-l4"><a class="reference internal" href="#parallel-pipeline">Parallel Pipeline</a></li>
<li class="toctree-l4"><a class="reference internal" href="#dist-pipeline">Dist Pipeline</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#computation-pattern">Computation Pattern</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id2">Serial Pipeline</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#off-policy-drl-dqn-impala-sac">Off-Policy DRL: DQN, IMPALA, SAC</a></li>
<li class="toctree-l4"><a class="reference internal" href="#on-policy-drl-ppo">On-Policy DRL: PPO</a></li>
<li class="toctree-l4"><a class="reference internal" href="#drl-rewardmodel-gail-her-rnd"><strong>DRL + RewardModel: GAIL, HER, RND</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="#drl-demostration-data-policy-r2d3-sqil-rbc"><strong>DRL + Demostration Data/Policy: R2D3, SQIL, RBC</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#parallel-dist-pipeline">Parallel/Dist Pipeline</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../intro_rl/index_zh.html">强化学习基础概念介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hands_on/index.html">Hands on RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../env_tutorial/index_zh.html">强化学习环境示例手册</a></li>
<li class="toctree-l1"><a class="reference internal" href="../best_practice/index.html">Best Practice</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc/index.html">API Doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index_zh.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../feature/index_zh.html">特性介绍</a></li>
</ul>
<p class="caption"><span class="caption-text">开发者指南</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../guide/index.html">开发者指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial_dev/index.html">Tutorial-Developer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../architecture/index.html">Architecture Design</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DI-engine</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Key Concept</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/key_concept/index.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="key-concept">
<h1>Key Concept<a class="headerlink" href="#key-concept" title="Permalink to this headline">¶</a></h1>
<div class="toctree-wrapper compound">
</div>
<p>Here we show some key concepts about reinforcement learning training and evaluation pipeline designed by DI-engine.
One of the basic control flows(serial pipeline) can be described as:</p>
<img alt="../_images/serial_pipeline.svg" class="align-center" src="../_images/serial_pipeline.svg" /><p>In the following sections, DI-engine first introduces key concepts/components separately, then combines them like building
a special “Evolution Graph” to offer different computation patterns(serial, parallel, dist).</p>
<div class="section" id="concept">
<h2>Concept<a class="headerlink" href="#concept" title="Permalink to this headline">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">Environment</span></code> and <code class="docutils literal notranslate"><span class="pre">policy</span></code> are the most two important concepts in the total design scheme, which can also be called description modules, in most cases, the users of DI-engine only need to pay
attention to these two components.</p>
<p><code class="docutils literal notranslate"><span class="pre">Worker</span></code> modules, such as <code class="docutils literal notranslate"><span class="pre">learner</span></code>, <code class="docutils literal notranslate"><span class="pre">collector</span></code>, and <code class="docutils literal notranslate"><span class="pre">buffer</span></code>, are execution modules implementing the corresponding tasks derived from the description modules. These worker
modules are general in many RL algorithms, but the users can also override their components easily, the only restriction is to obey the basic interface definition.</p>
<p>Last but not least, <code class="docutils literal notranslate"><span class="pre">config</span></code> is the recommended tool to control and record the whole pipeline.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Environment and policy are partially extended from the original definition in other RL papers and frameworks.</p>
</div>
<div class="section" id="environment">
<h3>Environment<a class="headerlink" href="#environment" title="Permalink to this headline">¶</a></h3>
<p>DI-engine environment is a superset of <code class="docutils literal notranslate"><span class="pre">gym.Env</span></code>, it is compatible with gym env interfaces and offers some optional interfaces, e.g.: dynamic seed, collect/evaluate setting, <a class="reference external" href="../feature/env_overview_en.html">Env Overview</a></p>
<p><code class="docutils literal notranslate"><span class="pre">EnvManager</span></code>, usually called Vectorized Environments in other frameworks, aims to implement parallel environment simulation to speed up data collection. Instead of interacting with 1 environment per collect step, it allows the collector to interact with N homogeneous environments per step, which means that <code class="docutils literal notranslate"><span class="pre">action</span></code> passed to <code class="docutils literal notranslate"><span class="pre">env.step</span></code> is a vector with a length of N, and the return value of <code class="docutils literal notranslate"><span class="pre">env.step</span></code> (obs, reward, done) is the same as it.</p>
<p>For the convenience of <strong>asynchronous reset</strong> and <strong>unifying asynchronous/synchronous step</strong>, DI-engine modifies the interface of env manager like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># DI-engine EnvManager                                                         # pseudo code in the other RL papers</span>
<span class="n">env</span><span class="o">.</span><span class="n">launch</span><span class="p">()</span>                                                                   <span class="c1"># obs = env.reset()</span>
<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>                                                                    <span class="c1"># while True:</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">ready_obs</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">random_policy</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>                                        <span class="c1">#     action = random_policy.forward(obs)</span>
    <span class="n">timestep</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>                                                <span class="c1">#     obs_, reward, done, info = env.step(action)</span>
    <span class="c1"># maybe some env_id matching when enable asynchronous</span>
    <span class="n">transition</span> <span class="o">=</span> <span class="p">[</span><span class="n">obs</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">timestep</span><span class="o">.</span><span class="n">obs</span><span class="p">,</span> <span class="n">timestep</span><span class="o">.</span><span class="n">reward</span><span class="p">,</span> <span class="n">timestep</span><span class="o">.</span><span class="n">done</span><span class="p">]</span>   <span class="c1">#     transition = [obs, action, obs_, reward, done]</span>
                                                                               <span class="c1">#     if done:</span>
                                                                               <span class="c1">#         obs[i] = env.reset(i)</span>
    <span class="k">if</span> <span class="n">env</span><span class="o">.</span><span class="n">done</span><span class="p">:</span>                                                               <span class="c1">#     if env.done  # collect enough env frames</span>
        <span class="k">break</span>                                                                  <span class="c1">#         break</span>
</pre></div>
</div>
<p>There are three types EnvManager in DI-engine now:</p>
<blockquote>
<div><ul class="simple">
<li><p>BaseEnvManager——<strong>local test and validation</strong></p></li>
<li><p>SyncSubprocessEnvManager——parallel simulation for <strong>low fluctuation environment</strong></p></li>
<li><p>AsyncSubprocessEnvManager——parallel simulation for <strong>high fluctuation environment</strong></p></li>
</ul>
</div></blockquote>
<p>The following demo image shows the detailed runtime logics between <code class="docutils literal notranslate"><span class="pre">BaseEnvManager</span></code> and <code class="docutils literal notranslate"><span class="pre">SyncSubprocessEnvManager</span></code>:</p>
<img alt="../_images/env_manager_base_sync.png" src="../_images/env_manager_base_sync.png" />
<p>For the subprocess-type env manager, DI-engine uses shared memory among different worker subprocesses to save the cost of IPC, and <a class="reference external" href="https://github.com/apache/arrow">pyarrow</a> will be a reliable alternative in the following version.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the environment is some kind of client, like SC2 and CARLA, maybe a new env manager based on python thread can be faster.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If there are some pre-defined neural networks in the environment using GPU, like the feature extractor VAE trained by self-supervised training before RL training, DI-engine recommends to utilize parallel executions in each subprocess rather than stack all the data in the main process and then forward this network. Moreover, it is not an elegant method, DI-engine will try to find some new flexible and general solution.</p>
</div>
<p>Besides, for robustness in practical usage, like IPC error(broken pipe, EOF) and environment runtime error, DI-engine also provides a series of <strong>Error Tolerance</strong> tools, e.g.: watchdog and auto-retry.</p>
<p>For all the mentioned features, the users can refer to <a class="reference external" href="../feature/env_manager_overview_en.html">EnvManager Overview</a> for more details.</p>
</div>
<div class="section" id="policy">
<h3>Policy<a class="headerlink" href="#policy" title="Permalink to this headline">¶</a></h3>
<p>To unify the design pattern and modularization of RL and other machine learning algorithms, DI-engine abstracts and defines the general policy interfaces with multi-mode design.
With these abstractions, plenty of the AI decision algorithms can be summarized in only one python file, i.e.: corresponding policy class. And the user’s customized algorithms only need to inherit and extend <code class="xref py py-class docutils literal notranslate"><span class="pre">Policy</span></code> or just have the same interface definition with it.</p>
<div class="section" id="the-multi-mode-of-policy">
<h4>The Multi-Mode of Policy<a class="headerlink" href="#the-multi-mode-of-policy" title="Permalink to this headline">¶</a></h4>
<p>In most cases, RL policy needs to execute different algorithm procedures for different usages, e.g.: for DQN, the model forward and calculating TD error in training,
the model forward without gradient computation and use epsilon-greedy to select actions for exploration in collecting. Therefore, DI-engine policy unifies all the algorithm content in only one python file,
prepares some simple interface methods, and combines them into 3 common modes——<strong>learn_mode, collect_mode, eval_mode</strong>, as is shown in the next image:</p>
<img alt="../_images/policy_mode.svg" src="../_images/policy_mode.svg" /><p>Learn_mode aims to policy updating, collect_mode does proper exploration and exploitation to collect training data, eval_mode evaluates policy performance clearly and fairly. And the users can customize their
own algorithm ideas by overriding these modes or design their customized modes, such as hyperparameters annealing according to training result, select battle players in self-play training, and so on. For more details,
the users can refer to <a class="reference external" href="../feature/policy_overview_en.html">Policy Overview</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">policy.learn_mode</span></code> is not the instance of <code class="xref py py-class docutils literal notranslate"><span class="pre">Policy</span></code> but a pure interface collection(implemented by namedtuple), which means the users can implement their policy class just ensuring the same method names and input/output arguments as the corresponding modes.</p>
</div>
</div>
<div class="section" id="shared-model-model-wrapper">
<h4>Shared Model + Model Wrapper<a class="headerlink" href="#shared-model-model-wrapper" title="Permalink to this headline">¶</a></h4>
<p>Neural network, often called model, is the one of most important components in the whole algorithm. For serial pipeline, the model is usually created in the public common constructor method(<code class="docutils literal notranslate"><span class="pre">__init__</span></code>) or out of policy and passed to the policy as arguments. Therefore, the model is shared among different modes for convenience. And DI-engine extends the model with more runtime function by <code class="docutils literal notranslate"><span class="pre">Model</span> <span class="pre">Wrapper</span></code> , which makes the shared model can exhibit different behaviors in different modes, such as sampling action by multinomial distribution in collect mode while <span class="math notranslate nohighlight">\(argmax\)</span> in evaluating mode. Here are some concrete code examples:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ding.model</span> <span class="kn">import</span> <span class="n">model_wrap</span><span class="p">,</span> <span class="n">DQN</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">DQN</span><span class="p">(</span><span class="n">obs_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">84</span><span class="p">,</span> <span class="mi">84</span><span class="p">),</span> <span class="n">action_shape</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
<span class="c1"># only wrapper, no model copy</span>
<span class="n">learn_model</span> <span class="o">=</span> <span class="n">model_wrap</span><span class="p">(</span><span class="n">model_wrap</span><span class="p">,</span> <span class="n">wrapper_name</span><span class="o">=</span><span class="s1">&#39;base&#39;</span><span class="p">)</span>
<span class="n">collector_model</span> <span class="o">=</span> <span class="n">model_wrap</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">wrapper_name</span><span class="o">=</span><span class="s1">&#39;multinomial_sample&#39;</span><span class="p">)</span>
<span class="n">eval_model</span> <span class="o">=</span> <span class="n">model_wrap</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">wrapper_name</span><span class="o">=</span><span class="s1">&#39;argmax_sample&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>If you want to know more about pre-defined models for each algorithms and customize you model, please refer to <a class="reference external" href="../feature/model_overview_en.html">Model Overview</a>.</p>
<p>If you want to know about the detailed information of the pre-defined model wrapper or customize your model wrapper, <a class="reference external" href="../feature/wrapper_hook_overview_en.html">Wrapper Overview</a> can help you a lot.</p>
</div>
<div class="section" id="processing-function">
<h4>Processing Function<a class="headerlink" href="#processing-function" title="Permalink to this headline">¶</a></h4>
<p>In practical algorithm implementations, the users often need to many data processing operations, like stacking several samples into a batch, data transformation between torch.Tensor and np.ndarray. As for RL
algorithms themselves, there are a great number of different styles of data pre-processing and aggregation, such as calculating N-step return and GAE(Generalized Advantage Estimation), split trajectories or unroll segments, and so on. Since then, DI-engine has provided some common processing functions, which can be called as a pure function. And the users can utilize these functions both in collect mode and in learning mode.</p>
<p>For example, where should we calculate advantages for some on-policy algorithms, such as A2C/PPO, learn mode or collect mode? The former can distribute computation to different collector nodes in distributed
training for saving time, and the latter can usually gain better performance due to more accurate approximation, just a trade-off. For a framework, it is more wiser to offer some powerful and efficient tools rather
than restricting some fixed pipelines. The following table shows some existing processing functions and related information:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 23%" />
<col style="width: 45%" />
<col style="width: 32%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Function Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Path</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>default_collate</p></td>
<td><p>Stack samples(dict/list/tensor) into batch</p></td>
<td><p>ding.utils.data.collate_fn</p></td>
</tr>
<tr class="row-odd"><td><p>default_decollate</p></td>
<td><p>Split batch into samples</p></td>
<td><p>ding.utils.data.collate_fn</p></td>
</tr>
<tr class="row-even"><td><p>get_nstep_return_data</p></td>
<td><p>Get nstep data(reward, next_obs, done)</p></td>
<td><p>ding.rl_utils.adder</p></td>
</tr>
<tr class="row-odd"><td><p>get_gae</p></td>
<td><p>Get GAE advantage</p></td>
<td><p>ding.rl_utils.adder</p></td>
</tr>
<tr class="row-even"><td><p>to_tensor</p></td>
<td><p>Transform data to torch.Tensor</p></td>
<td><p>ding.torch_utils.data_helper</p></td>
</tr>
<tr class="row-odd"><td><p>to_device</p></td>
<td><p>Transform device(cpu or cuda)</p></td>
<td><p>ding.torch_utils.data_helper</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="scale-up-to-parallel-training">
<h4>Scale Up to Parallel Training<a class="headerlink" href="#scale-up-to-parallel-training" title="Permalink to this headline">¶</a></h4>
<p>TBD</p>
</div>
</div>
<div class="section" id="config">
<h3>Config<a class="headerlink" href="#config" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id1">
<h4>key concept<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h4>
<p>Config module is a component for users to determine what kind of parameters they want to use.
The overall design is as follows:</p>
<img alt="" src="../_images/config.png" />
<p>As you can see from above diagram, the entire config is mainly from two parts. One is called Default Config,
which is our recommended setting for policy and env, and may not change a lot. The other is called User Config,
which users may want to specify case by case.</p>
<p>In order to get the entire config, we have <strong>compile</strong> phase, a bottom-up process. First we get the default setting
for each submodule like Learner, Collector, etc. Then we put them together and get Default Config
for policy and env. Finally, we merge Default config with User Config and get the entire config.</p>
<p>On the other hand, <strong>initialization</strong> phase, the process to create modules according to config, is from top to bottom. We will start from
policy and env, and then pass configs to each execution modules.</p>
<p>In DI-engine, we write config as a python <code class="docutils literal notranslate"><span class="pre">dict</span></code>. Below is an outline for Default Config.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cartpole_dqn_default_config</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">env</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="n">manager</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="o">...</span><span class="p">),</span>
        <span class="o">...</span>
    <span class="p">),</span>
    <span class="n">policy</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="o">...</span><span class="p">),</span>
        <span class="n">collect</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="o">...</span><span class="p">),</span>
        <span class="n">learn</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="o">...</span><span class="p">),</span>
        <span class="nb">eval</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="o">...</span><span class="p">),</span>
        <span class="n">other</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
            <span class="n">replay_buffer</span><span class="o">=</span><span class="nb">dict</span><span class="p">(),</span>
            <span class="o">...</span>
        <span class="p">),</span>
        <span class="o">...</span>
    <span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="config-overview">
<h4>config overview<a class="headerlink" href="#config-overview" title="Permalink to this headline">¶</a></h4>
<p>In the following table, we list some commonly-used keys as well as their meanings.
For policy-related keys, please refer to the document <a class="reference external" href="../hands_on/index.html">Hands On
RL</a> section.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 46%" />
<col style="width: 54%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Key</p></th>
<th class="head"><p>Meaning</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>policy.batch_size</p></td>
<td><p>(int) number of data for a training
iteration</p></td>
</tr>
<tr class="row-odd"><td><p>policy.update_per_collect</p></td>
<td><p>(int) after getting training data,
leaner will update model for
update_per_collect times</p></td>
</tr>
<tr class="row-even"><td><p>policy.n_sample</p></td>
<td><p>(int) number of samples that will
be sent to replay_buffer from
collector</p></td>
</tr>
<tr class="row-odd"><td><p>policy.nstep</p></td>
<td><p>(int) number of steps that will be
used when calculating TD-error.</p></td>
</tr>
<tr class="row-even"><td><p>policy.cuda</p></td>
<td><p>(bool) whether to use cuda when
training</p></td>
</tr>
<tr class="row-odd"><td><p>policy.priority</p></td>
<td><p>(bool) whether to use priority
replay buffer</p></td>
</tr>
<tr class="row-even"><td><p>policy.on_policy</p></td>
<td><p>(bool) whether to use on policy
training</p></td>
</tr>
<tr class="row-odd"><td><p>env.stop_value</p></td>
<td><p>(int) when reward exceeds
env.stop_value, stop training</p></td>
</tr>
<tr class="row-even"><td><p>env.collector_env_num</p></td>
<td><p>(int) number of environments to
collect data when training</p></td>
</tr>
<tr class="row-odd"><td><p>env.evaluator_env_num</p></td>
<td><p>(int) number of environments to
collect data when evaluating</p></td>
</tr>
</tbody>
</table>
<p>Rules when merging user-specific config and predefined config:</p>
<ul class="simple">
<li><p>User-specific config is of highest priority, which means that it will cover
the predefined one when conflict occurs.</p></li>
<li><p>Some important keys, such as <code class="docutils literal notranslate"><span class="pre">env.stop_value</span></code>, <code class="docutils literal notranslate"><span class="pre">env.n_evaluator_episode</span></code>, <code class="docutils literal notranslate"><span class="pre">policy.on_policy</span></code>,
<code class="docutils literal notranslate"><span class="pre">policy.collect.n_sample</span></code> or <code class="docutils literal notranslate"><span class="pre">policy.collect.n_episode</span></code>  must be specific.</p></li>
<li><p>The merged entire config will be saved to <code class="docutils literal notranslate"><span class="pre">total_config.py</span></code> and <code class="docutils literal notranslate"><span class="pre">formatted_total_config.py</span></code>  by default.</p></li>
</ul>
</div>
<div class="section" id="how-to-customize">
<span id="header-n125"></span><h4>How to customize?<a class="headerlink" href="#how-to-customize" title="Permalink to this headline">¶</a></h4>
<p>Suppose we need to set key <code class="docutils literal notranslate"><span class="pre">nstep</span></code> mentioned above to 3, how to do it?</p>
<p>If the file name of user config is <code class="docutils literal notranslate"><span class="pre">dqn_user_config.py</span></code>, just add the following code into User config.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">policy</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
    <span class="o">...</span><span class="p">,</span>
    <span class="n">learn</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="o">...</span><span class="p">,</span>
        <span class="n">nstep</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>After writing the user config, we can run our DQN experiment according
to <a class="reference external" href="../quick_start/index.html">Quick Start</a>.</p>
</div>
</div>
<div class="section" id="worker-collector">
<h3>Worker-Collector<a class="headerlink" href="#worker-collector" title="Permalink to this headline">¶</a></h3>
<p>Collector is one of the most important components among all the workers, which is often called <code class="docutils literal notranslate"><span class="pre">actor</span></code> in other frameworks and DI-engine renames it to distinguish with actor-critic. It aims to offer sufficient
quantity and quality data for policy training (learner). And collector is only responsible for data collection but decoupled with data management, that is to say, it returns collected trajectories directly and
these data can be used for training directly or pushed into replay buffer.</p>
<p>There are 3 core parts for a collector——env manager, policy(collect_mode), collector controller, and these parts can be implemented in a
single process or located in several machines. Usually, DI-engine use a multi-process env_manager and another main loop controller process with policy to construct a collector, which may be extended in the future.</p>
<p>Due to different send/receive data logic, the collector now is divided into two patterns——serial and parallel, we will introduce them separately.</p>
<div class="section" id="serial-collector">
<h4>Serial Collector<a class="headerlink" href="#serial-collector" title="Permalink to this headline">¶</a></h4>
<p>From the viewpoint of the basic unit of collecting data, sample(step) and episode are two mainly used types. Therefore, DI-engine defines the abstract interfaces <code class="docutils literal notranslate"><span class="pre">ISerialCollector</span></code> for a serial collector and
implements <code class="docutils literal notranslate"><span class="pre">SampleCollector</span></code> and <code class="docutils literal notranslate"><span class="pre">EpisodeCollector</span></code>, which covers almost RL usages but the users can also easily customize when encountering some special demands.</p>
<img alt="../_images/serial_collector_class.svg" class="align-center" src="../_images/serial_collector_class.svg" /><p>The core usage of collector is quite simple, the users just need to create a corresponding type collector and indicate <code class="docutils literal notranslate"><span class="pre">n_sample</span></code> or <code class="docutils literal notranslate"><span class="pre">n_episode</span></code> as the argument of <code class="docutils literal notranslate"><span class="pre">collect</span></code> method. Here is a naive example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">from</span> <span class="nn">easydict</span> <span class="kn">import</span> <span class="n">EasyDict</span>
<span class="kn">from</span> <span class="nn">ding.policy</span> <span class="kn">import</span> <span class="n">DQNPolicy</span>
<span class="kn">from</span> <span class="nn">ding.env</span> <span class="kn">import</span> <span class="n">BaseEnvManager</span>
<span class="kn">from</span> <span class="nn">ding.worker</span> <span class="kn">import</span> <span class="n">SampleCollector</span><span class="p">,</span> <span class="n">EpisodeCollector</span>

<span class="c1"># prepare components</span>
<span class="n">cfg</span><span class="p">:</span> <span class="n">EasyDict</span>  <span class="c1"># config after `compile_config`</span>
<span class="n">normal_env</span> <span class="o">=</span> <span class="n">BaseEnvManager</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>  <span class="c1"># normal vectorized env</span>

<span class="n">dqn_policy</span> <span class="o">=</span> <span class="n">DQNPolicy</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="p">)</span>
<span class="n">sample_collector</span> <span class="o">=</span> <span class="n">SampleCollector</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">collect</span><span class="o">.</span><span class="n">collector</span><span class="p">,</span> <span class="n">normal_env</span><span class="p">,</span> <span class="n">dqn_policy</span><span class="o">.</span><span class="n">collect_mode</span><span class="p">)</span>
<span class="n">episode_collector</span> <span class="o">=</span> <span class="n">EpisodeCollector</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">collect</span><span class="o">.</span><span class="n">collector</span><span class="p">,</span> <span class="n">normal_env</span><span class="p">,</span> <span class="n">dqn_policy</span><span class="o">.</span><span class="n">collect_mode</span><span class="p">)</span>

<span class="c1"># collect 100 train sample</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">sample_collector</span><span class="o">.</span><span class="n">collect</span><span class="p">(</span><span class="n">n_sample</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">==</span> <span class="mi">100</span>
<span class="k">assert</span> <span class="nb">all</span><span class="p">([</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">data</span><span class="p">])</span>

<span class="c1"># collect 10 env episode</span>
<span class="n">episodes</span> <span class="o">=</span> <span class="n">episode_collector</span><span class="o">.</span><span class="n">collect</span><span class="p">(</span><span class="n">n_episode</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">episodes</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">episodes</span><span class="p">)</span> <span class="o">==</span> <span class="mi">10</span>

<span class="c1"># push into replay buffer/send to learner/data preprocessing</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For all cases, the number of collect data, n_sample/n_episode, is fixed in the total training procedure, so our example codes set this field in configs, such as <code class="docutils literal notranslate"><span class="pre">config.policy.collect.n_sample</span></code>.</p>
</div>
<p>The structure and main loop of collector can be summarized as the next image, the interaction of policy and env consists of <code class="docutils literal notranslate"><span class="pre">policy.forward</span></code>, <code class="docutils literal notranslate"><span class="pre">env.step</span></code> and the related support codes. Then <code class="docutils literal notranslate"><span class="pre">policy.process_transition</span></code> and
<code class="docutils literal notranslate"><span class="pre">policy.get_train_sample</span></code> contributes to process data into training samples and pack them to a list. For <code class="docutils literal notranslate"><span class="pre">EpisodeCollector</span></code>, which is usually used in some cases that need to do special post-processing,
<code class="docutils literal notranslate"><span class="pre">policy.get_train_sample</span></code> is disabled and the users can do anything after receiving the collected data.</p>
<img alt="../_images/collector_pipeline.svg" class="align-center" src="../_images/collector_pipeline.svg" /><p>Sometimes, we use different policies even different envs to collect data, such as using random policy at the beginning of training to prepare warmup data, and calculate distillation loss with the probability of
expert policy. And all the demands can be implemented by <code class="docutils literal notranslate"><span class="pre">reset_policy</span></code>, <code class="docutils literal notranslate"><span class="pre">reset_env</span></code>, <code class="docutils literal notranslate"><span class="pre">reset</span></code> method like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># prepare components</span>
<span class="n">dqn_policy</span> <span class="o">=</span> <span class="n">DQNPolicy</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">random_policy</span> <span class="o">=</span> <span class="n">RandomPolicy</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">expert_policy</span> <span class="o">=</span> <span class="n">ExpertPolicy</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

<span class="n">collector</span> <span class="o">=</span> <span class="n">SampleCollector</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">replay_buffer</span> <span class="o">=</span> <span class="n">NaiveBuffer</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

<span class="c1"># train beginning(random_policy)</span>
<span class="n">collector</span><span class="o">.</span><span class="n">reset_policy</span><span class="p">(</span><span class="n">random_policy</span><span class="o">.</span><span class="n">collect_mode</span><span class="p">)</span>
<span class="n">random_data</span> <span class="o">=</span> <span class="n">collector</span><span class="o">.</span><span class="n">collect</span><span class="p">(</span><span class="n">n_sample</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">replay_buffer</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="n">random_data</span><span class="p">)</span>
<span class="c1"># main loop</span>
<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="o">...</span>
    <span class="n">collector</span><span class="o">.</span><span class="n">reset_policy</span><span class="p">(</span><span class="n">dqn_policy</span><span class="o">.</span><span class="n">collect_mode</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">collector</span><span class="o">.</span><span class="n">collect</span><span class="p">(</span><span class="n">n_sample</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">collector</span><span class="o">.</span><span class="n">reset_policy</span><span class="p">(</span><span class="n">expert_policy</span><span class="o">.</span><span class="n">collect_mode</span><span class="p">)</span>
    <span class="n">expert_data</span> <span class="o">=</span> <span class="n">collector</span><span class="o">.</span><span class="n">collect</span><span class="p">(</span><span class="n">n_sample</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="c1"># train dqn_policy with collected data</span>
    <span class="o">...</span>
</pre></div>
</div>
<p>Besides, serial collector shows less difference between on-policy and off-policy algorithms, the only thing is to reset some statistics and temporal buffers, which can be automatically executed by collector, the
users just need to ensure the correct value of <code class="docutils literal notranslate"><span class="pre">config.policy.on_policy</span></code>.</p>
<p>Last, there are some other features such as collecting data with asynchronous env_manager, dealing with abnormal env steps, please refer to <a class="reference external" href="../feature/collector_overview_en.html">Collector Overview</a>.</p>
</div>
<div class="section" id="parallel-collector">
<h4>Parallel Collector<a class="headerlink" href="#parallel-collector" title="Permalink to this headline">¶</a></h4>
<p>TBD</p>
</div>
</div>
<div class="section" id="worker-buffer">
<h3>Worker-Buffer<a class="headerlink" href="#worker-buffer" title="Permalink to this headline">¶</a></h3>
<p>Replay buffer is a component to store data collected by collector or generated by a fixed policy(usually expert policy), then provide data for the learner to optimize policy. In DI-engine, there are <strong>three types of replay buffers</strong>:</p>
<blockquote>
<div><ul class="simple">
<li><p>NaiveReplayBuffer</p></li>
<li><p>AdvancedReplayBuffer</p></li>
<li><p>EpisodeReplayBuffer</p></li>
</ul>
</div></blockquote>
<p>These three are all subclasses derived from abstract Interface <code class="docutils literal notranslate"><span class="pre">IBuffer</span></code>.</p>
<a class="reference internal image-reference" href="../_images/buffer_class_uml.png"><img alt="../_images/buffer_class_uml.png" class="align-center" src="../_images/buffer_class_uml.png" style="width: 862.0px; height: 467.0px;" /></a>
<p>The key methods of a buffer are <code class="docutils literal notranslate"><span class="pre">push</span></code> and <code class="docutils literal notranslate"><span class="pre">sample</span></code>. <code class="docutils literal notranslate"><span class="pre">NaiveReplayBuffer</span></code> is a simple FIFO queue implementation. It only provides basic functions of the two methods.</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">push</span></code>: Push some collected data in the buffer. If exceeding the max size of the buffer, queue head data will be removed out of buffer.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sample</span></code>: Uniformly sample a list with length <cite>batch_size</cite> by random.</p></li>
</ol>
</div></blockquote>
<p>Based on <code class="docutils literal notranslate"><span class="pre">NaiveReplayBuffer</span></code>, <code class="docutils literal notranslate"><span class="pre">AdvancedReplayBuffer</span></code> and <code class="docutils literal notranslate"><span class="pre">EpisodeReplayBuffer</span></code> respectively implements more functions and features.</p>
<p><code class="docutils literal notranslate"><span class="pre">AdvancedReplayBuffer</span></code> implements the following features: (Also shown in the figure)</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>Prioritized Sampling</strong>. Completely implement paper <a class="reference external" href="https://arxiv.org/abs/1511.05952">Prioritized Experience Replay</a></p></li>
<li><p><strong>Monitor data quality(use_count and staleness)</strong>. If a piece of data is used too many times or is too stale to optimize policy, it will be removed out of the buffer.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>use_count</strong>: Count how many times a piece of data is sampled.</p>
<p><strong>staleness</strong>: Model iteration gap between the time when it is collected and the time when it is sampled</p>
</div>
<ul class="simple">
<li><p><strong>Throughput monitor and control</strong>. In a fixed period, count how many pieces of data are pushed into, sampled out of, removed out of the buffer. Control the ratio “Pushed in” / “Sampled out” in a range, in case the dataflow speed does not match.</p></li>
<li><p><strong>Logger</strong>. Sampled data attributes and throughput are shown in text logger and tensorboard logger.</p></li>
</ul>
</div></blockquote>
<a class="reference internal image-reference" href="../_images/advanced_buffer.png"><img alt="../_images/advanced_buffer.png" class="align-center" src="../_images/advanced_buffer.png" style="width: 767.0px; height: 441.35px;" /></a>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>By default, most policies in DI-engine adopt <code class="docutils literal notranslate"><span class="pre">AdvancedReplayBuffer</span></code>, because we think monitor and logger are rather important in debugging and policy tuning. However, if you are sure that you do not need all the features above, you can feel free to switch to simpler and faster <code class="docutils literal notranslate"><span class="pre">NaiveReplayBuffer</span></code>.</p>
</div>
<p><code class="docutils literal notranslate"><span class="pre">EpisodeReplayBuffer</span></code> is designed for some special cases where they need a whole episode rather than separated samples. For example: In chess, go or card games, players get reward only when the game is over; In some algorithms like <a class="reference external" href="https://arxiv.org/abs/1707.01495">Hindsight Experience Replay</a>, must sample out a whole episode and operate on it. Therefore, in <code class="docutils literal notranslate"><span class="pre">EpisodeReplayBuffer</span></code>, each element is no longer a training sample, but an episode.</p>
<p>In DI-engine, we define <strong>full data</strong> and <strong>meta data</strong>. <strong>Full data</strong> is often a dict, with keys <code class="docutils literal notranslate"><span class="pre">['obs',</span> <span class="pre">'action',</span> <span class="pre">'next_obs',</span> <span class="pre">'reward',</span> <span class="pre">'info']</span></code> and some optional keys like <code class="docutils literal notranslate"><span class="pre">['priority',</span> <span class="pre">'use_count',</span> <span class="pre">'collect_iter',</span> <span class="pre">...]</span></code>. However, in some complex environments(Usually we run them in parallel mode), <code class="docutils literal notranslate"><span class="pre">['obs',</span> <span class="pre">'action',</span> <span class="pre">'next_obs',</span> <span class="pre">'reward',</span> <span class="pre">'info']</span></code> can be too big to store in memory. Therefore, we store them in file system, and only store <strong>meta data</strong> including <code class="docutils literal notranslate"><span class="pre">'file_path'</span></code> and optional keys in memory. Therefore, in parallel mode, when removing the data out of buffer, we must not only remove meta data in memory but also remove that in the file system as well.</p>
<p>If you want to know more details about the three types of replay buffers, or the remove mechanism in parallel mode, please refer to <a class="reference external" href="../feature/replay_buffer_overview_en.html">Replay Buffer Overview</a></p>
</div>
<div class="section" id="worker-evaluator">
<h3>Worker-Evaluator<a class="headerlink" href="#worker-evaluator" title="Permalink to this headline">¶</a></h3>
<p>Evaluator, another key execution component of DI-engine, is used to determine whether the training model is convergent
or not. Similar to collector, evaluator consists of three key components ——env manager, policy(eval_mode), evaluator
controller.</p>
<p>Env manager allow us to run multiple environments one by one(<code class="docutils literal notranslate"><span class="pre">base_env_manager`)</span> <span class="pre">or</span> <span class="pre">in</span> <span class="pre">parallel</span>
<span class="pre">(``subprocess_env_manager</span></code>). For example, if we use subprocess env manager, we will run different environments
in different subprocesses, which will greatly increase the efficiency of collecting episodes.</p>
<p>Policy(eval_mode) is the rl model which we need to check.</p>
<p>Evaluator controller is a component to determine if we should stop evaluating or not. For example, in <code class="docutils literal notranslate"><span class="pre">Serial</span>
<span class="pre">Evaluator</span></code>, <code class="docutils literal notranslate"><span class="pre">n_evaluator_episode</span></code> is an argument to determine how many episodes we want to collect and evaluate. Once
we collect these number of episodes, evaluator will stop collecting and start to compute the average reward. If the average
is larger than <code class="docutils literal notranslate"><span class="pre">stop_value</span></code>, <code class="docutils literal notranslate"><span class="pre">stop_flag</span></code> will be True, and we will know our model is already convergent.</p>
<div class="section" id="serial-evaluator">
<h4>Serial Evaluator<a class="headerlink" href="#serial-evaluator" title="Permalink to this headline">¶</a></h4>
<p>Serial evaluator is used in serial pipeline. Key concepts are <code class="docutils literal notranslate"><span class="pre">n_evaluator_episode</span></code> and <code class="docutils literal notranslate"><span class="pre">stop_value</span></code>, which has been
explained above.</p>
<p>The following is an example of how to use serial evaluator:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">from</span> <span class="nn">easydict</span> <span class="kn">import</span> <span class="n">EasyDict</span>
<span class="kn">from</span> <span class="nn">ding.policy</span> <span class="kn">import</span> <span class="n">DQNPolicy</span>
<span class="kn">from</span> <span class="nn">ding.env</span> <span class="kn">import</span> <span class="n">BaseEnvManager</span>
<span class="kn">from</span> <span class="nn">ding.worker</span> <span class="kn">import</span> <span class="n">BaseSerialEvaluator</span>

<span class="c1"># prepare components</span>
<span class="n">cfg</span><span class="p">:</span> <span class="n">EasyDict</span>  <span class="c1"># config after `compile_config`</span>
<span class="n">normal_env</span> <span class="o">=</span> <span class="n">BaseEnvManager</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>  <span class="c1"># normal vectorized env</span>

<span class="n">dqn_policy</span> <span class="o">=</span> <span class="n">DQNPolicy</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="p">)</span>
<span class="n">evaluator</span> <span class="o">=</span> <span class="n">BaseSerialEvaluator</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">eval</span><span class="o">.</span><span class="n">evaluator</span><span class="p">,</span> <span class="n">normal_env</span><span class="p">,</span> <span class="n">dqn_policy</span><span class="o">.</span><span class="n">eval_mode</span><span class="p">)</span>

<span class="c1"># evalulate 10 env episode</span>
<span class="n">stop</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="n">evaluator</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">,</span> <span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">,</span> <span class="n">collector</span><span class="o">.</span><span class="n">envstep</span><span class="p">,</span> <span class="n">n_episode</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">reward</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span> <span class="o">==</span> <span class="mi">10</span>

<span class="c1"># judge whether the return value reaches the convergence standard</span>
<span class="k">if</span> <span class="n">stop</span><span class="p">:</span>
   <span class="k">break</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Different environments may have different  <code class="docutils literal notranslate"><span class="pre">stop_value</span></code> and <code class="docutils literal notranslate"><span class="pre">n_evaluator_episode</span></code>. For example, in cartpole,
we have``stop_value=195`` and <code class="docutils literal notranslate"><span class="pre">n_evaluator_episode=100</span></code>. Users should indicate these two arguments in <code class="docutils literal notranslate"><span class="pre">env</span></code>
config(i.e. <code class="docutils literal notranslate"><span class="pre">env.stop_value</span></code>, <code class="docutils literal notranslate"><span class="pre">env.n_evaluator_episode</span></code>), and then they will be passed to evaluator.</p>
</div>
<p>Combined with the evaluation condition(i.e. <code class="docutils literal notranslate"><span class="pre">should_eval</span></code> method), We can add the evaluator into the serial pipeline as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iterations</span><span class="p">):</span>

    <span class="c1"># Evaluate policy performance</span>
    <span class="k">if</span> <span class="n">evaluator</span><span class="o">.</span><span class="n">should_eval</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">):</span>
        <span class="c1">#load model</span>
        <span class="n">stop</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="n">evaluator</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">,</span> <span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">,</span> <span class="n">collector</span><span class="o">.</span><span class="n">envstep</span><span class="p">)</span>
        <span class="c1"># if stop flag, exit the process</span>
        <span class="k">if</span> <span class="n">stop</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="c1"># if not stop flag, continue to collect data and train the model</span>
        <span class="n">new_data</span> <span class="o">=</span> <span class="n">collector</span><span class="o">.</span><span class="n">collect</span><span class="p">(</span><span class="n">train_iter</span><span class="o">=</span><span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">,</span> <span class="n">policy_kwargs</span><span class="o">=</span><span class="n">collect_kwargs</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><strong>How to judge whether the model converges or not?</strong></p>
<p>We judge whether the model converges or not based on average reward. In DI-engine, there are three types of average
reward: winning probability, total cumulative reward and average unit step reward.</p>
<p>Winning probability: In games like <code class="docutils literal notranslate"><span class="pre">SMAC</span></code>, we focus on the final result and don’t care too much about the
game process. For such environments, we use winning probability(for <code class="docutils literal notranslate"><span class="pre">SMAC</span></code> 1.0 in 3s5z) as the convergence condition.</p>
<p>Total cumulative reward: In games like <code class="docutils literal notranslate"><span class="pre">cartpole</span></code> and <code class="docutils literal notranslate"><span class="pre">lunarlander</span></code>, we need to make the total score as large as
possible. So we use total cumulative reward as the convergence condition.</p>
<p>Average unit step reward: In some games, we need to make the total reward as large as possible and reduce the number
of unnecessary exploration steps in the meantime. For such environments, we use average unit step reward as the
convergence condition.</p>
<p>Besides, a reliable RL experiment should be repeated 3~5 times with different random seeds, and some statistics
such as the median value and the mean/std value can be more convincing.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><strong>How to solve the problem that different environment in evaluator may collect different length episode?</strong></p>
<p>In some cases, this is really a big problem. For example, suppose we want to collect 12 episodes in evaluator
but only have 5 environments, if we didn’t do any thing, it is likely that we will get more short episodes than long
episodes. As a result, our average reward will have a bias and may not be accurate. This is obvious since short
episodes need less time.</p>
<p>In order to solve such problem, we use <code class="docutils literal notranslate"><span class="pre">VectorEvalMonitor</span></code>, a component to balance how many episodes to collect
per environment. Let’s go back to the above example, we will collect three episodes for either of the first two
environments but only two for each of the remaining environments.</p>
<p>Besides, we use <code class="docutils literal notranslate"><span class="pre">get_episode_reward</span></code> to get the sum of the rewards of k episodes in each environment, and
<code class="docutils literal notranslate"><span class="pre">get_current_episode</span></code> to get the episode num k in each environment.</p>
</div>
</div>
</div>
<div class="section" id="worker-learner">
<h3>Worker-Learner<a class="headerlink" href="#worker-learner" title="Permalink to this headline">¶</a></h3>
<p>Learner is one of the most important components among all the workers, who is responsible for optimizing the policy by training data. Unlike another important component <code class="docutils literal notranslate"><span class="pre">Collector</span></code>, learner is not divided into serial and parallel modes, i.e. There is only one learner class, serial and parallel entry can call different methods for training.</p>
<p><strong>Serial pipeline</strong> would call learner’s <code class="docutils literal notranslate"><span class="pre">train</span></code> method for training. <code class="docutils literal notranslate"><span class="pre">train</span></code> method receives a batch of data, and call learn_mode policy’s <code class="docutils literal notranslate"><span class="pre">_forward_learn</span></code> to train for one iteraton.</p>
<p><strong>Parallel pipeline</strong> would call learner’s <code class="docutils literal notranslate"><span class="pre">start</span></code> method for a complete process of training. <code class="docutils literal notranslate"><span class="pre">start</span></code> method has a loop, which includes fetching data from source(Often file system), and calling <code class="docutils literal notranslate"><span class="pre">train</span></code> for one-iteration training. <code class="docutils literal notranslate"><span class="pre">start</span></code> will train for a specific number of iterations, which is set by use config.</p>
<p>Besides <code class="docutils literal notranslate"><span class="pre">train</span></code> and <code class="docutils literal notranslate"><span class="pre">start</span></code>, learner also provides a useful interface called <code class="docutils literal notranslate"><span class="pre">save_checkpoint</span></code>, which can save current state_dict as a checkpoint during training.</p>
<p>In learner, there is a special concept called <code class="docutils literal notranslate"><span class="pre">Hook</span></code>. Hook is responsible for doing some fixed jobs at specific timings, including “before_run”(at the beginning of <code class="docutils literal notranslate"><span class="pre">start</span></code> ), “after_run”(at the ending of <code class="docutils literal notranslate"><span class="pre">start</span></code> ), “before_iter”(at the beginning of <code class="docutils literal notranslate"><span class="pre">train</span></code> ), “after_iter”(at the ending of <code class="docutils literal notranslate"><span class="pre">train</span></code> ).</p>
<p>Hook has many different types. DI-engine now has hooks to save checkpoint( <code class="docutils literal notranslate"><span class="pre">save_checkpoint</span></code> also uses this hook), load checkpoint, print log(text &amp; tb), reduce log from multiple learners. Users can also implement their own hooks easily. If you want to know more about hook mechanism, you can refer to <a class="reference external" href="../feature/wrapper_hook_overview_en.html">Wrapper &amp; Hook Overview</a>.</p>
<p>For more details about learner, please refer to <a class="reference external" href="../feature/learner_overview_en.html">Learner Overview</a>.</p>
</div>
<div class="section" id="entry">
<h3>Entry<a class="headerlink" href="#entry" title="Permalink to this headline">¶</a></h3>
<p>DI-engine offers 3 training entries for different usage, users can choose any one they like:</p>
<div class="section" id="serial-pipeline">
<h4>Serial Pipeline<a class="headerlink" href="#serial-pipeline" title="Permalink to this headline">¶</a></h4>
<blockquote>
<div><p>There 3 types entries, and users can select anyone they like in practice. Different entries are designed for various demand.</p>
<ol class="arabic">
<li><p>CLI</p>
<blockquote>
<div><p><strong>Simply run a training program, validate correctness, acquire RL model or expert data.</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># usage 1(without config)</span>
ding -m serial -e cartpole -p dqn --train-iter <span class="m">100000</span> -s <span class="m">0</span>
<span class="c1"># usage 2(with config)</span>
ding -m serial -c cartpole_dqn_config.py -s <span class="m">0</span>
</pre></div>
</div>
<p>You can enter in <code class="docutils literal notranslate"><span class="pre">ding</span> <span class="pre">-h</span></code> for more information.</p>
</div></blockquote>
</li>
<li><p>Customized Main Function</p>
<blockquote>
<div><p><strong>Customize you RL training pipeline, design algorithm or apply it in your environment.</strong></p>
<p>refer to some example main function python file in <code class="docutils literal notranslate"><span class="pre">dizoo/envname/entry/envname_policyname_main.py</span></code> , such as:</p>
<blockquote>
<div><ul class="simple">
<li><p>dizoo/classic_control/cartpole/entry/cartpole_dqn_main.py</p></li>
<li><p>dizoo/classic_control/cartpole/entry/cartpole_ppo_main.py</p></li>
<li><p>dizoo/classic_control/pendulum/entry/pendulum_td3_main.py</p></li>
</ul>
</div></blockquote>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3 -u cartpole_dqn_main.py  <span class="c1"># users can also add arguments list in your own entry file</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Unified Entry Function</p>
<blockquote>
<div><p><strong>Config-mode entry, just adjust hyper-parameters and do comparsion experiements in the existing algorithms and pipelines.</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ding.entry</span> <span class="kn">import</span> <span class="n">serial_pipeline</span>
<span class="kn">from</span> <span class="nn">dizoo.classic_control.cartpole.config.cartpole_dqn_config</span> <span class="kn">import</span> <span class="n">main_config</span><span class="p">,</span> <span class="n">create_config</span>
<span class="n">serial_pipeline</span><span class="p">([</span><span class="n">main_config</span><span class="p">,</span> <span class="n">create_config</span><span class="p">],</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>You can refer to <code class="docutils literal notranslate"><span class="pre">ding/entry</span></code> directory and read related entry functions and tests.</p>
</div></blockquote>
</li>
</ol>
</div></blockquote>
</div>
<div class="section" id="parallel-pipeline">
<h4>Parallel Pipeline<a class="headerlink" href="#parallel-pipeline" title="Permalink to this headline">¶</a></h4>
<blockquote>
<div><ol class="arabic simple">
<li><p>CLI</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># config path: dizoo/classic_control/cartpole/config/parallel/cartpole_dqn_config.py</span>
ding -m parallel -c cartpole_dqn_config.py -s <span class="m">0</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Unified Entry Function</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ding.entry</span> <span class="kn">import</span> <span class="n">parallel_pipeline</span>
<span class="kn">from</span> <span class="nn">dizoo.classic_control.cartpole.config.parallel.cartpole_dqn_config</span> <span class="kn">import</span> <span class="n">main_config</span><span class="p">,</span> <span class="n">create_config</span><span class="p">,</span> <span class="n">system_config</span>
<span class="n">parallel_pipeline</span><span class="p">([</span><span class="n">main_config</span><span class="p">,</span> <span class="n">create_config</span><span class="p">,</span> <span class="n">system_config</span><span class="p">],</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</div>
<div class="section" id="dist-pipeline">
<h4>Dist Pipeline<a class="headerlink" href="#dist-pipeline" title="Permalink to this headline">¶</a></h4>
<blockquote>
<div><ol class="arabic simple">
<li><p>CLI for local</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># config path: dizoo/classic_control/cartpole/config/parallel/cartpole_dqn_config.py</span>
<span class="nb">export</span> <span class="nv">PYTHONUNBUFFERED</span><span class="o">=</span><span class="m">1</span>
ding -m dist --module config -p <span class="nb">local</span> -c cartpole_dqn_config.py -s <span class="m">0</span>
ding -m dist --module learner --module-name learner0 -c cartpole_dqn_config.py.pkl -s <span class="m">0</span> <span class="p">&amp;</span>
ding -m dist --module collector --module-name collector0 -c cartpole_dqn_config.py.pkl -s <span class="m">0</span> <span class="p">&amp;</span>
ding -m dist --module collector --module-name collector1 -c cartpole_dqn_config.py.pkl -s <span class="m">0</span> <span class="p">&amp;</span>
ding -m dist --module coordinator -p <span class="nb">local</span> -c cartpole_dqn_config.py.pkl -s <span class="m">0</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>CLI for server(such as SLURM)</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># config path: dizoo/classic_control/cartpole/config/parallel/cartpole_dqn_config.py</span>
<span class="nb">export</span> <span class="nv">PYTHONUNBUFFERED</span><span class="o">=</span><span class="m">1</span>
<span class="nv">learner_host</span><span class="o">=</span><span class="m">10</span>-10-10-10
<span class="nv">collector_host</span><span class="o">=</span><span class="m">10</span>-10-10-<span class="o">[</span><span class="m">11</span>-12<span class="o">]</span>
<span class="nv">partition</span><span class="o">=</span><span class="nb">test</span>

ding -m dist --module config -p slurm -c cartpole_dqn_config.py -s <span class="m">0</span> -lh <span class="nv">$learner_host</span> -clh <span class="nv">$collector_host</span>
srun -p <span class="nv">$partition</span> -w <span class="nv">$learner_host</span> --gres<span class="o">=</span>gpu:1 ding -m dist --module learner --module-name learner0 -c cartpole_dqn_config.py.pkl -s <span class="m">0</span> <span class="p">&amp;</span>
srun -p <span class="nv">$partition</span> -w <span class="nv">$collector_host</span> ding -m dist --module collector --module-name collector0 -c cartpole_dqn_config.py.pkl -s <span class="m">0</span> <span class="p">&amp;</span>
srun -p <span class="nv">$partition</span> -w <span class="nv">$collector_host</span> ding -m dist --module collector --module-name collector1 -c cartpole_dqn_config.py.pkl -s <span class="m">0</span> <span class="p">&amp;</span>
ding -m dist --module coordinator -p slurm -c cartpole_dqn_config.py.pkl -s <span class="m">0</span>
</pre></div>
</div>
<ol class="arabic" start="3">
<li><p>CLI for k8s</p>
<blockquote>
<div><p>TBD</p>
</div></blockquote>
</li>
</ol>
</div></blockquote>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you want to know more details about algorithm implementation, framework design, and efficiency optimization, we also provide the documentation of <a class="reference external" href="../feature/index.html">Feature</a>,</p>
</div>
</div>
</div>
</div>
<div class="section" id="computation-pattern">
<h2>Computation Pattern<a class="headerlink" href="#computation-pattern" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id2">
<h3>Serial Pipeline<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<div class="section" id="off-policy-drl-dqn-impala-sac">
<h4>Off-Policy DRL: DQN, IMPALA, SAC<a class="headerlink" href="#off-policy-drl-dqn-impala-sac" title="Permalink to this headline">¶</a></h4>
<img alt="../_images/serial_pipeline.svg" class="align-center" src="../_images/serial_pipeline.svg" /><p>Users can easily implement various DRL algorithms by combining and utilizing different description and execution modules in DI-engine, here are some demostration designs:</p>
</div>
<div class="section" id="on-policy-drl-ppo">
<h4>On-Policy DRL: PPO<a class="headerlink" href="#on-policy-drl-ppo" title="Permalink to this headline">¶</a></h4>
<p><strong>Changes</strong>: Remove buffer</p>
<img alt="../_images/serial_pipeline_on_policy.svg" class="align-center" src="../_images/serial_pipeline_on_policy.svg" /></div>
<div class="section" id="drl-rewardmodel-gail-her-rnd">
<h4><strong>DRL + RewardModel: GAIL, HER, RND</strong><a class="headerlink" href="#drl-rewardmodel-gail-her-rnd" title="Permalink to this headline">¶</a></h4>
<p><strong>Changes</strong>: Add reward model and related data transformation</p>
<img alt="../_images/serial_pipeline_reward_model.svg" class="align-center" src="../_images/serial_pipeline_reward_model.svg" /></div>
<div class="section" id="drl-demostration-data-policy-r2d3-sqil-rbc">
<h4><strong>DRL + Demostration Data/Policy: R2D3, SQIL, RBC</strong><a class="headerlink" href="#drl-demostration-data-policy-r2d3-sqil-rbc" title="Permalink to this headline">¶</a></h4>
<p><strong>Changes</strong>: Add expert data buffer(demo buffer) or collector described by expert policy</p>
<img alt="../_images/serial_pipeline_r2d3.svg" class="align-center" src="../_images/serial_pipeline_r2d3.svg" /></div>
</div>
<div class="section" id="parallel-dist-pipeline">
<h3>Parallel/Dist Pipeline<a class="headerlink" href="#parallel-dist-pipeline" title="Permalink to this headline">¶</a></h3>
<p><strong>Changes</strong>: Coordinator and objstore, policy flow, data flow(meta and step) and task flow</p>
<img alt="../_images/parallel_pipeline.svg" class="align-center" src="../_images/parallel_pipeline.svg" /></div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../intro_rl/index_zh.html" class="btn btn-neutral float-right" title="强化学习基础概念介绍" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../quick_start/index_zh.html" class="btn btn-neutral float-left" title="快速上手" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, OpenDILab Contributors

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>