

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Quick Start &mdash; DI-engine 0.1.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Key Concept" href="../key_concept/index.html" />
    <link rel="prev" title="Installation" href="../installation/index.html" />
    <link href="../_static/css/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> DI-engine
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../installation/index.html">Installation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Quick Start</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#instantiate-the-run-time-config">Instantiate the run-time config</a></li>
<li class="toctree-l2"><a class="reference internal" href="#initialize-the-environments">Initialize the Environments</a></li>
<li class="toctree-l2"><a class="reference internal" href="#set-up-the-policy-and-nn-models">Set up the Policy and NN models</a></li>
<li class="toctree-l2"><a class="reference internal" href="#define-the-execution-modules">Define the Execution Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="#aggregate-the-training-and-evaluation-pipelines">Aggregate the Training and Evaluation Pipelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="#other-utilities">Other Utilities</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#epsilon-greedy">Epsilon Greedy</a></li>
<li class="toctree-l3"><a class="reference internal" href="#visualization-logging">Visualization &amp; Logging</a></li>
<li class="toctree-l3"><a class="reference internal" href="#loading-saving-checkpoints">Loading &amp; Saving checkpoints</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../key_concept/index.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro_rl/index.html">Introduction to RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../env_tutorial/index.html">RL Environment Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hands_on/index.html">Hands on RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../best_practice/index.html">Best Practice</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc/index.html">API Doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index_en.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../feature/index.html">Feature</a></li>
</ul>
<p class="caption"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../guide/index.html">Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial_dev/index.html">Tutorial-Developer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../architecture/index.html">Architecture Design</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DI-engine</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Quick Start</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/quick_start/index.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="quick-start">
<h1>Quick Start<a class="headerlink" href="#quick-start" title="Permalink to this headline">¶</a></h1>
<div class="toctree-wrapper compound">
</div>
<img alt="../_images/cartpole_cmp.gif" class="align-center" src="../_images/cartpole_cmp.gif" />
<p>As kickoff, we will illustrate how to launch a RL experiment on a simple <code class="docutils literal notranslate"><span class="pre">CartPole</span></code> environment (as shown in above figure) using DI-engine.</p>
<p>Concretely, we will define a training pipeline in a single python file that specifies the training hyper-parameters, sampling and evaluation environments, the neural networks for the RL agents, as well as the training workflow.</p>
<div class="section" id="instantiate-the-run-time-config">
<h2>Instantiate the run-time config<a class="headerlink" href="#instantiate-the-run-time-config" title="Permalink to this headline">¶</a></h2>
<p>The first step to build a training workflow is to specify the training configuration.
DI-engine prefers a nested python dict object to represent all parameters and configurations of an RL experiment, for example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cartpole_dqn_config</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">exp_name</span><span class="o">=</span><span class="s2">&quot;cartpole_dqn&quot;</span><span class="p">,</span>
    <span class="n">env</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="n">collector_env_num</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
        <span class="n">evaluator_env_num</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">policy</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
            <span class="n">encoder_hidden_size_list</span><span class="o">=</span><span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">],</span>
        <span class="p">),</span>
        <span class="n">discount_factor</span><span class="o">=</span><span class="mf">0.97</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="o">......</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the specific example, you can refer to:</p>
<blockquote>
<div><ul class="simple">
<li><p>config:  <code class="docutils literal notranslate"><span class="pre">dizoo/classic_control/cartpole/config/cartpole_dqn_config.py</span></code></p></li>
<li><p>main: <code class="docutils literal notranslate"><span class="pre">dizoo/classic_control/cartpole/entry/cartpole_dqn_main.py</span></code></p></li>
</ul>
</div></blockquote>
<p>and you just need to run this experiment following the next command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3 -u dizoo/classic_control/cartpole/entry/cartpole_dqn_main.py
</pre></div>
</div>
</div>
<p>DI-engine provides default configs for all modules, and also a helper function <code class="docutils literal notranslate"><span class="pre">compile_config</span></code> to merge the default configs of these modules into a run-time config object (a <code class="docutils literal notranslate"><span class="pre">EasyDict</span></code> object that can be accessed by string key <code class="docutils literal notranslate"><span class="pre">cfg[&quot;env&quot;]</span></code> or attribute <code class="docutils literal notranslate"><span class="pre">cfg.env</span></code>):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ding.config</span> <span class="kn">import</span> <span class="n">compile_config</span>
<span class="kn">from</span> <span class="nn">ding.envs</span> <span class="kn">import</span> <span class="n">BaseEnvManager</span><span class="p">,</span> <span class="n">DingEnvWrapper</span>
<span class="kn">from</span> <span class="nn">ding.model</span> <span class="kn">import</span> <span class="n">DQN</span>
<span class="kn">from</span> <span class="nn">ding.policy</span> <span class="kn">import</span> <span class="n">DQNPolicy</span>
<span class="kn">from</span> <span class="nn">ding.worker</span> <span class="kn">import</span> <span class="n">BaseLearner</span><span class="p">,</span> <span class="n">SampleCollector</span><span class="p">,</span> <span class="n">BaseSerialEvaluator</span><span class="p">,</span> <span class="n">AdvancedReplayBuffer</span>
<span class="kn">from</span> <span class="nn">dizoo.classic_control.cartpole.config.cartpole_dqn_config</span> <span class="kn">import</span> <span class="n">cartpole_dqn_config</span>

<span class="c1"># compile config</span>
<span class="n">cfg</span> <span class="o">=</span> <span class="n">compile_config</span><span class="p">(</span>
    <span class="n">cartpole_dqn_config</span><span class="p">,</span>
    <span class="n">BaseEnvManager</span><span class="p">,</span>
    <span class="n">DQNPolicy</span><span class="p">,</span>
    <span class="n">BaseLearner</span><span class="p">,</span>
    <span class="n">SampleCollector</span><span class="p">,</span>
    <span class="n">BaseSerialEvaluator</span><span class="p">,</span>
    <span class="n">AdvancedReplayBuffer</span><span class="p">,</span>
    <span class="n">save_cfg</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
<p>In this example, we only present the procedure to specify config in the entry file. In the following section, we construct the RL pipeline in the same entry file based on the specified config.</p>
<p>Please note that DI-engine also supports running a RL experiment directly according to a given config file, e.g.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ding -m serial -c cartpole_dqn_config.py -s <span class="m">0</span>
</pre></div>
</div>
<p>For more design details, please refer to the <a class="reference external" href="../key_concept/index.html#config">Config</a> section and <a class="reference external" href="../key_concept/index.html#entry">Entry</a>.</p>
</div>
<div class="section" id="initialize-the-environments">
<h2>Initialize the Environments<a class="headerlink" href="#initialize-the-environments" title="Permalink to this headline">¶</a></h2>
<p>The RL agents interact with the environment to collect training data or test its performance.
DI-engine provides enhanced RL environment interfaces derived from the widely used <a class="reference external" href="https://github.com/openai/gym">OpenAI Gym</a>.
You can simply wrap the gym environment into DI-engine environment by using the environment warpper <code class="xref py py-class docutils literal notranslate"><span class="pre">DingEnvWrapper</span></code>.
You can also construct a more complex environment class following the guidelines in <a class="reference external" href="../key_concept/index.html#env">Environment</a> section.</p>
<p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">Env</span> <span class="pre">Manager</span></code> is used to manage multiple vectorized environments, usually implemented by
multi-processes parallelly. The interfaces of <cite>env manager</cite> are similar to those of a simple gym env. Here we show a case
of using <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseEnvManager</span></code> to build environments for collection and evaluation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>

<span class="k">def</span> <span class="nf">wrapped_cartpole_env</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">DingEnvWrapper</span><span class="p">(</span><span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;CartPole-v0&#39;</span><span class="p">))</span>

<span class="n">collector_env_num</span><span class="p">,</span> <span class="n">evaluator_env_num</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">collector_env_num</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">evaluator_env_num</span>
<span class="n">collector_env</span> <span class="o">=</span> <span class="n">BaseEnvManager</span><span class="p">(</span><span class="n">env_fn</span><span class="o">=</span><span class="p">[</span><span class="n">wrapped_cartpole_env</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">collector_env_num</span><span class="p">)],</span> <span class="n">cfg</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">manager</span><span class="p">)</span>
<span class="n">evaluator_env</span> <span class="o">=</span> <span class="n">BaseEnvManager</span><span class="p">(</span><span class="n">env_fn</span><span class="o">=</span><span class="p">[</span><span class="n">wrapped_cartpole_env</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">evaluator_env_num</span><span class="p">)],</span> <span class="n">cfg</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">manager</span><span class="p">)</span>
</pre></div>
</div>
<p>In order to ensure the reproducibility of experiement, we setup the seed of environments and common packages.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ding.utils</span> <span class="kn">import</span> <span class="n">set_pkg_seed</span>

<span class="n">collector_env</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">evaluator_env</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dynamic_seed</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">set_pkg_seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">use_cuda</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">cuda</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="set-up-the-policy-and-nn-models">
<h2>Set up the Policy and NN models<a class="headerlink" href="#set-up-the-policy-and-nn-models" title="Permalink to this headline">¶</a></h2>
<p>DI-engine supports most of the common policies used in RL training. Each is defined as a <code class="xref py py-class docutils literal notranslate"><span class="pre">Policy</span></code>
class. The details of optimiaztion algorithm, data pre-processing and post-processing, usage of neural networks
are encapsulated inside. Users only need to build a PyTorch network structure and pass into the policy.</p>
<p>DI-engine also provides default networks to simply apply to the environment. For some complex RL methods, users can imitate the interfaces of these default models and customize own networks.</p>
<p>For example, a <code class="docutils literal notranslate"><span class="pre">DQN</span></code> policy for <code class="docutils literal notranslate"><span class="pre">CartPole</span></code> can be defined as follow.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">DQN</span><span class="p">(</span><span class="o">**</span><span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">DQNPolicy</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="define-the-execution-modules">
<h2>Define the Execution Modules<a class="headerlink" href="#define-the-execution-modules" title="Permalink to this headline">¶</a></h2>
<p>DI-engine needs to build some execution components to manage an RL training procedure.
A <code class="xref py py-class docutils literal notranslate"><span class="pre">Collector</span></code> is used to sample and provide data for training.
A <code class="xref py py-class docutils literal notranslate"><span class="pre">Learner</span></code> is used to receive training data and conduct
the training (including updating networks, strategy and etc.).
An <code class="xref py py-class docutils literal notranslate"><span class="pre">Evaluator</span></code> is build to perform the evaluation when needed.
And other components like <code class="xref py py-class docutils literal notranslate"><span class="pre">Replay</span> <span class="pre">Buffer</span></code> may be required for the
training process. All these module can be customized by config or rewritten by the user.</p>
<p>An example of setting all the above is showed as follow.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">tensorboardX</span> <span class="kn">import</span> <span class="n">SummaryWriter</span>

<span class="n">tb_logger</span> <span class="o">=</span> <span class="n">SummaryWriter</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;./</span><span class="si">{}</span><span class="s1">/log/&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">exp_name</span><span class="p">),</span> <span class="s1">&#39;serial&#39;</span><span class="p">))</span>
<span class="n">learner</span> <span class="o">=</span> <span class="n">BaseLearner</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">learner</span><span class="p">,</span> <span class="n">policy</span><span class="o">.</span><span class="n">learn_mode</span><span class="p">,</span> <span class="n">tb_logger</span><span class="p">,</span> <span class="n">exp_name</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">exp_name</span><span class="p">)</span>
<span class="n">collector</span> <span class="o">=</span> <span class="n">SampleCollector</span><span class="p">(</span>
    <span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">collect</span><span class="o">.</span><span class="n">collector</span><span class="p">,</span> <span class="n">collector_env</span><span class="p">,</span> <span class="n">policy</span><span class="o">.</span><span class="n">collect_mode</span><span class="p">,</span> <span class="n">tb_logger</span><span class="p">,</span> <span class="n">exp_name</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">exp_name</span>
<span class="p">)</span>
<span class="n">evaluator</span> <span class="o">=</span> <span class="n">BaseSerialEvaluator</span><span class="p">(</span>
    <span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">eval</span><span class="o">.</span><span class="n">evaluator</span><span class="p">,</span> <span class="n">evaluator_env</span><span class="p">,</span> <span class="n">policy</span><span class="o">.</span><span class="n">eval_mode</span><span class="p">,</span> <span class="n">tb_logger</span><span class="p">,</span> <span class="n">exp_name</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">exp_name</span>
<span class="p">)</span>
<span class="n">replay_buffer</span> <span class="o">=</span> <span class="n">AdvancedReplayBuffer</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">other</span><span class="o">.</span><span class="n">replay_buffer</span><span class="p">,</span> <span class="n">tb_logger</span><span class="p">,</span> <span class="n">exp_name</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">exp_name</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="aggregate-the-training-and-evaluation-pipelines">
<h2>Aggregate the Training and Evaluation Pipelines<a class="headerlink" href="#aggregate-the-training-and-evaluation-pipelines" title="Permalink to this headline">¶</a></h2>
<p>The training loop in DI-engine can be customized arbitrarily. Usually the training process may consist of
collecting data, updating policy, updating related modules and evaluation.</p>
<p>Here we provide examples of off-policy training (<code class="docutils literal notranslate"><span class="pre">DQN</span></code>) for a <code class="docutils literal notranslate"><span class="pre">CartPole</span></code> environment. For more algorithms, you can refer to dizoo.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ding.rl_utils</span> <span class="kn">import</span> <span class="n">get_epsilon_greedy_fn</span>

<span class="c1"># DQN training loop</span>
<span class="n">eps_cfg</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">other</span><span class="o">.</span><span class="n">eps</span>
<span class="n">epsilon_greedy</span> <span class="o">=</span> <span class="n">get_epsilon_greedy_fn</span><span class="p">(</span><span class="n">eps_cfg</span><span class="o">.</span><span class="n">start</span><span class="p">,</span> <span class="n">eps_cfg</span><span class="o">.</span><span class="n">end</span><span class="p">,</span> <span class="n">eps_cfg</span><span class="o">.</span><span class="n">decay</span><span class="p">,</span> <span class="n">eps_cfg</span><span class="o">.</span><span class="n">type</span><span class="p">)</span>
<span class="n">max_iterations</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">1e8</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iterations</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">evaluator</span><span class="o">.</span><span class="n">should_eval</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">):</span>
        <span class="n">stop</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="n">evaluator</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">,</span> <span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">,</span> <span class="n">collector</span><span class="o">.</span><span class="n">envstep</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">stop</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">epsilon_greedy</span><span class="p">(</span><span class="n">collector</span><span class="o">.</span><span class="n">envstep</span><span class="p">)</span>
    <span class="n">new_data</span> <span class="o">=</span> <span class="n">collector</span><span class="o">.</span><span class="n">collect</span><span class="p">(</span><span class="n">train_iter</span><span class="o">=</span><span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">,</span> <span class="n">policy_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;eps&#39;</span><span class="p">:</span> <span class="n">eps</span><span class="p">})</span>
    <span class="n">replay_buffer</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="n">new_data</span><span class="p">,</span> <span class="n">cur_collector_envstep</span><span class="o">=</span><span class="n">collector</span><span class="o">.</span><span class="n">envstep</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">update_per_collect</span><span class="p">):</span>
        <span class="n">train_data</span> <span class="o">=</span> <span class="n">replay_buffer</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">get_attribute</span><span class="p">(</span><span class="s1">&#39;batch_size&#39;</span><span class="p">),</span> <span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">train_data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">learner</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">collector</span><span class="o">.</span><span class="n">envstep</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The users can refer to the complete demo in <code class="docutils literal notranslate"><span class="pre">dizoo/classic_control/cartpole/entry/cartpole_dqn_main.py</span></code>.</p>
</div>
</div>
<div class="section" id="other-utilities">
<h2>Other Utilities<a class="headerlink" href="#other-utilities" title="Permalink to this headline">¶</a></h2>
<p>DI-engine supports various useful tools in common RL training, as shown in follows.</p>
<div class="section" id="epsilon-greedy">
<h3>Epsilon Greedy<a class="headerlink" href="#epsilon-greedy" title="Permalink to this headline">¶</a></h3>
<p>An easy way of deploying epsilon greedy exploration when sampling data is shown as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ding.rl_utils</span> <span class="kn">import</span> <span class="n">get_epsilon_greedy_fn</span>

<span class="n">eps_cfg</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">other</span><span class="o">.</span><span class="n">eps</span>
<span class="n">epsilon_greedy</span> <span class="o">=</span> <span class="n">get_epsilon_greedy_fn</span><span class="p">(</span><span class="n">eps_cfg</span><span class="o">.</span><span class="n">start</span><span class="p">,</span> <span class="n">eps_cfg</span><span class="o">.</span><span class="n">end</span><span class="p">,</span> <span class="n">eps_cfg</span><span class="o">.</span><span class="n">decay</span><span class="p">,</span> <span class="n">eps_cfg</span><span class="o">.</span><span class="n">type</span><span class="p">)</span>
<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">epsilon_greedy</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">)</span>
    <span class="o">...</span>
</pre></div>
</div>
<p>Firstly, you should call <code class="docutils literal notranslate"><span class="pre">get_epsilon_greedy_fn</span></code> to acquire an eps-greedy function. Then, you should call <code class="docutils literal notranslate"><span class="pre">epsilon_greedy</span></code> function at each step. The epsilon decay strategy can be configured by you, for example, start value, end value, type of decay(linear, exponential). And you can control whether it decay by env step or train iteration.</p>
</div>
<div class="section" id="visualization-logging">
<h3>Visualization &amp; Logging<a class="headerlink" href="#visualization-logging" title="Permalink to this headline">¶</a></h3>
<p>Some environments have a rendering visualization. DI-engine doesn’t use render interface, but supports saving replay videos instead.
After training, users can add the code shown below to enable this function. If everything works well, you can find some videos with <code class="docutils literal notranslate"><span class="pre">.mp4</span></code> suffix in directory <code class="docutils literal notranslate"><span class="pre">replay_path</span></code> (some GUI interfaces are normal).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">evaluator_env</span> <span class="o">=</span> <span class="n">BaseEnvManager</span><span class="p">(</span><span class="n">env_fn</span><span class="o">=</span><span class="p">[</span><span class="n">wrapped_cartpole_env</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">evaluator_env_num</span><span class="p">)],</span> <span class="n">cfg</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">manager</span><span class="p">)</span>
<span class="n">cfg</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">replay_path</span> <span class="o">=</span> <span class="s1">&#39;./video&#39;</span>  <span class="c1"># indicate save replay directory path</span>
<span class="n">evaluator_env</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dynamic_seed</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">evaluator_env</span><span class="o">.</span><span class="n">enable_save_replay</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">replay_path</span><span class="p">)</span>  <span class="c1"># switch save replay interface</span>
<span class="n">evaluator</span> <span class="o">=</span> <span class="n">BaseSerialEvaluator</span><span class="p">(</span>
    <span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">eval</span><span class="o">.</span><span class="n">evaluator</span><span class="p">,</span> <span class="n">evaluator_env</span><span class="p">,</span> <span class="n">policy</span><span class="o">.</span><span class="n">eval_mode</span><span class="p">,</span> <span class="n">tb_logger</span><span class="p">,</span> <span class="n">exp_name</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">exp_name</span>
<span class="p">)</span>
<span class="n">evaluator</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">,</span> <span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">,</span> <span class="n">collector</span><span class="o">.</span><span class="n">envstep</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If users want to visualize with a trained policy, please refer to <code class="docutils literal notranslate"><span class="pre">dizoo/classic_control/cartpole/entry/cartpole_dqn_eval.py</span></code> to construct a user-defined evaluation function, and indicate two fields <code class="docutils literal notranslate"><span class="pre">env.replay_path</span></code> and <code class="docutils literal notranslate"><span class="pre">policy.learn.learner.hook.load_ckpt_before_run</span></code> in config. An example is shown as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">env</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="n">replay_path</span><span class="o">=</span><span class="s1">&#39;your_replay_save_dir_path&#39;</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">policy</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="o">...</span><span class="p">,</span>
        <span class="n">load_path</span><span class="o">=</span><span class="s1">&#39;your_ckpt_path&#39;</span><span class="p">,</span>
        <span class="o">...</span><span class="p">,</span>
    <span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>All new RL environments can define their own <code class="docutils literal notranslate"><span class="pre">enable_save_replay</span></code> method to specify how to generate replay files. DI-engine utilizes <code class="docutils literal notranslate"><span class="pre">gym</span> <span class="pre">wrapper</span> <span class="pre">(coupled</span> <span class="pre">with</span> <span class="pre">ffmpeg)</span></code> to generate replays for some traditional environments. If users encounter some errors in recording videos by <code class="docutils literal notranslate"><span class="pre">gym</span> <span class="pre">wrapper</span></code>, you should install <code class="docutils literal notranslate"><span class="pre">ffmpeg</span></code> first.</p>
</div>
<p>Similar with other Deep Learning platforms, DI-engine uses tensorboard to record key parameters and results during
training. In addition to the default logging parameters, users can add their own logging parameters as follow.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tb_logger</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s1">&#39;epsilon_greedy&#39;</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">)</span>
</pre></div>
</div>
<p>If you want to know more details about default information recorded in tensorboard, see our
<a class="reference external" href="./tb_demo.html">tensorboard and logging demo</a> for a
DQN experiment.</p>
</div>
<div class="section" id="loading-saving-checkpoints">
<h3>Loading &amp; Saving checkpoints<a class="headerlink" href="#loading-saving-checkpoints" title="Permalink to this headline">¶</a></h3>
<p>It is usually needed to save and resume an experiment with model checkpoint.
DI-engine saves and loads checkpoints in the same way as PyTorch.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ckpt_path</span> <span class="o">=</span> <span class="s1">&#39;path/to/your/ckpt&#39;</span>
<span class="n">state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">ckpt_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="n">learner</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
<span class="n">learner</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> load ckpt in </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">ckpt_path</span><span class="p">))</span>

<span class="o">...</span>

<span class="n">dirname</span> <span class="o">=</span> <span class="s1">&#39;./ckpt_</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">dirname</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ckpt_name</span> <span class="o">=</span> <span class="s1">&#39;iteration_</span><span class="si">{}</span><span class="s1">.pth.tar&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">last_iter</span><span class="o">.</span><span class="n">val</span><span class="p">)</span>
<span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dirname</span><span class="p">,</span> <span class="n">ckpt_name</span><span class="p">)</span>
<span class="n">state_dict</span> <span class="o">=</span> <span class="n">learner</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>
<span class="n">learner</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> save ckpt in </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">path</span><span class="p">))</span>
</pre></div>
</div>
<p>To deploy this in a more elegant way, DI-engine is configured to use
<a class="reference internal" href="../api_doc/worker/learner/learner.html#ding.worker.learner.learner_hook.LearnerHook" title="ding.worker.learner.learner_hook.LearnerHook"><code class="xref py py-class docutils literal notranslate"><span class="pre">Learner</span> <span class="pre">Hook</span></code></a> to handle these cases. The saving hook is
automatically called after training iterations. And to load &amp; save checkpoints at the beginning and
in the end, users can simply add one-line code before &amp; after training as follows.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">learner</span><span class="o">.</span><span class="n">call_hook</span><span class="p">(</span><span class="s1">&#39;before_run&#39;</span><span class="p">)</span>

<span class="c1"># training loop</span>
<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="o">...</span>

<span class="n">learner</span><span class="o">.</span><span class="n">call_hook</span><span class="p">(</span><span class="s1">&#39;after_run&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>For more information, please take a look to <a class="reference external" href="../feature/wrapper_hook_overview.html">Wrapper &amp; Hook Overview</a> doc.</p>
<p>(Note: This page is based on commit )</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../key_concept/index.html" class="btn btn-neutral float-right" title="Key Concept" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../installation/index.html" class="btn btn-neutral float-left" title="Installation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, OpenDILab Contributors

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>