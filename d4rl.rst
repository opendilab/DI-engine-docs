D4RL (Mujoco)
~~~~~~~~~~~~~~

Overview
===========
D4RL is an open-source benchmark for offline Reinforcement Learning that provides standardized environments and datasets for training and benchmarking algorithms. The collection strategy for the dataset includes

1. Datasets generated by hand-designed rules and expert demonstrations
2. Multi-task datasets (agents perform different tasks in the same environment)
3. Datasets collected using mixed strategies

Specifically, it includes the following 7 sub-environments

- Maze2D
- AntMaze
- Gym-MuJoco
- Adroit
- FrankaKitchen
- Flow
- Offline CARLA

Note: offline rl is a dataset of d4rl for training, and testing is to interact with a specific RL environment, such as Mujoco.

The Mujoco dataset is a physics engine designed to promote research and development in fields such as robotics, biomechanics, graphics, and animation that require fast and accurate simulation. It is often used as a benchmarking environment for continuous space reinforcement learning algorithms. It is a collection of 20 sub-environments. In D4RL, the sub-environments used are Half Cheetah, Hopper, Walker2D.
Each sub-environment contains 5 mini-environments

- expert: train a \ `SAC <https://arxiv.org//abs/1801.01290>`__\ algorithm online until the strategy reaches the expert performance level, using the expert strategy to collect 1 million sample data
- medium-expert: mixes equal amounts of data collected by expert and medium strategies
- medium: first train a SAC algorithm online, stop training in the middle, and then use this partially trained strategy to collect 1 million sample data
- medium-replay: trains a SAC algorithm online until the policy reaches a medium performance level, collecting all samples placed in the buffer during training
- random: use a random initialization strategy to collect

The picture below shows one of the Hopper games.

.. image:: ./images/d4rl.gif
   :align: center

Install
====

Installation Method
--------
Just install the d4rl, gym and mujoco-py libraries, in which d4rl can be installed with one click through pip or through clone

.. code :: shell

    # pip install
    pip install git+https://github.com/rail-berkeley/d4rl@master#egg=d4rl

    # installed by cloning the repository
    git clone https://github.com/rail-berkeley/d4rl.git
    cd d4rl
    pip install -e .




Mujoco only needs the gym and mujoco-py two libraries, which can be installed by one-click pip or combined with DI-engine

1. The mujoco-py library no longer requires an activation license (``mujoco-py>=2.1.0``), you can install free-mujoco-py via \ `pip <https://github.com/openai/ mujoco-py/pull/640>`__ 

2. If you want to install ``mujoco-py>=2.1``, you can do the following:

.. code :: shell
    
    # Installation for Linux
    # Download the MuJoCo version 2.1 binaries for Linux.
    wget https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz
    # Extract the downloaded mujoco210 directory into ~/.mujoco/mujoco210.
    tar xvf mujoco210-linux-x86_64.tar.gz && mkdir -p ~/.mujoco && mv mujoco210 ~/.mujoco/mujoco210
    # Install and use mujoco-py
    pip install gym
    pip install -U 'mujoco-py<2.2,>=2.1'

    # Installation for macOS
    # Download the MuJoCo version 2.1 binaries for OSX.
    wget https://mujoco.org/download/mujoco210-macos-x86_64.tar.gz
    # Extract the downloaded mujoco210 directory into ~/.mujoco/mujoco210.
    tar xvf mujoco210-macos-x86_64.tar.gz && mkdir -p ~/.mujoco && mv mujoco210 ~/.mujoco/mujoco210
    # Install and use mujoco-py
    pip install gym
    pip install -U 'mujoco-py<2.2,>=2.1'
    
3. If you want to install ``mujoco-py<2.1``, you can do the following:

.. code :: shell

    # Installation for Linux
    # Download the MuJoCo version 2.0 binaries for Linux.
    wget https://www.roboti.us/download/mujoco200_linux.zip
    # Extract the downloaded mujoco200 directory into ~/.mujoco/mujoco200.
    unzip mujoco200_linux.zip && mkdir -p ~/.mujoco && mv mujoco200_linux ~/.mujoco/mujoco200
    # Download unlocked activation key.
    wget https://www.roboti.us/file/mjkey.txt -O ~/.mujoco/mjkey.txt
    # Install and use mujoco-py
    pip install gym
    pip install -U 'mujoco-py<2.1'

    # Installation for macOS
    # Download the MuJoCo version 2.0 binaries for OSX.
    wget https://www.roboti.us/download/mujoco200_macos.zip
    # Extract the downloaded mujoco200 directory into ~/.mujoco/mujoco200.
    tar xvf mujoco200-macos-x86_64.tar.gz && mkdir -p ~/.mujoco && mv mujoco200_macos ~/.mujoco/mujoco200
    # Download unlocked activation key.
    wget https://www.roboti.us/file/mjkey.txt -O ~/.mujoco/mjkey.txt
    # Install and use mujoco-py
    pip install gym
    pip install -U 'mujoco-py<2.1'


Verify Installation
---------------------

After the installation is complete, you can verify that the installation was successful by running the following command on the Python command line:

.. code :: python

    import gym
    import d4rl # Import required to register environments

    # Create the environment
    env = gym.make('maze2d-umaze-v1')

    # d4rl abides by the OpenAI gym interface
    env.reset()
    env.step(env.action_space.sample())

    # Each task is associated with a dataset
    # dataset contains observations, actions, rewards, terminals, and infos
    dataset = env.get_dataset()
    print(dataset['observations']) # An N x dim_observation Numpy array of observations

    # Alternatively, use d4rl.qlearning_dataset which
    # also adds next_observations.
    dataset = d4rl.qlearning_dataset(env)

Mirror
-------

DI-engine has prepared an mirror ready with the framework, available via \ ``docker pull opendilab/ding:nightly-mujoco``\, or by accessing \ `docker
hub <https://hub.docker.com/repository/docker/opendilab/ding>`__\ for more image

.._spatial original environment before transformation):

Gym-MuJoco space before transformation (original environment)
===============================================================


Observation Space
---------------------

- A vector composed of physical information (3D position, orientation, and joint angles etc. ), the specific size is \ ``(N, )``\ , where \ ``N``\ is determined according to the environment, and the data type is \ ``float64``\
- `Fujimoto <https://github.com/opendilab/DI-engine/blob/main/dizoo/d4rl/entry/d4rl_cql_main.py>`__ mentioned that doing obs norm for d4rl dataset will improve offline training stability sex


Action Space
----------------

- A vector composed of physical information (torque etc.), generally a continuous action space of size N (N varies with the specific sub-environment), the data type is \ ``float32``\, and an np array needs to be passed in (for example, the action is \ ``array([-0.9266078 , -0.4958926 , 0.46242517], dtype=float32)``\ )

- For example, in the Hopper environment, the size of N is 3, and the action takes the value in  \ ``[-1, 1]``\


Bonus Space
--------

- Depending on the specific game content, the game score will vary greatly, usually a \float\ value. For the specific value, please refer to the benchmark algorithm performance section at the bottom.


Other
----

- The end of the game is the end of the current environment episode

Quick Facts
========

1. Vector physical information input, empirically it is not appropriate to subtract the mean value in norm

2. Continuous action space

3. Dense rewards

4. The scale of reward value varies greatly

.._transformed spatial rl environment):

Transformed Space (RL Environment)
=======================


Observation Space
--------

- Basically no transformation


Action Space
--------

- Basically no transformation, it is still a continuous action space of size N, the value range is \ ``[-1, 1]``\, the size is \ ``(N, )``\ , and the data type is  \ ``np.float32``\


Bonus Space
--------

- Basically no transformation

The above space can be expressed as:

.. code :: python

   import gym


   obs_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(11, ), dtype=np.float64)
   act_space = gym.spaces.Box(low=-1, high=1, shape=(3, ), dtype=np.float32)
   rew_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(1, ), dtype=np.float32)


Other
----

- The \ ``info``\ returned by the environment \ ``step``\ method must contain the \ ``final_eval_reward``\ key-value pair, which represents the evaluation index of the entire episode, and is the cumulative sum of the rewards of the entire episode in Mujoco

Other
====

Lazy Initialization
----------------------

In order to facilitate parallel operations such as environment vectorization, environment instances generally implement lazy initialization, that is, the \ ``__init__``\ method does not initialize the real original environment instance, but only sets relevant parameters and configuration values. The concrete original environment instance is initialized when the ``reset``\ method is used.


Store Video
--------------

After the environment is created, but before reset, use the  \ ``enable_save_replay``\ method to specify the path to save the game recording. The environment will automatically save the local video files after each episode ends. (The default call \ ``gym.wrapper.Monitor``\ implementation, depends on \ ``ffmpeg``\), the code shown below will run an environment episode and save the result of this episode in the form  \ ``./video/xxx.mp4``\ in a file like this:

.. code :: python

   from easydict import EasyDict
   from dizoo.mujoco.envs import MujocoEnv

   env = MujocoEnv(EasyDict({'env_id': 'Hooper-v3' }))
   env.enable_save_replay(replay_path='./video')
   obs = env.reset()

   while True:
       action = env.random_action()
       timestep = env.step(action)
       if timestep.done:
           print('Episode is over, final eval reward is: {}'.format(timestep.info['final_eval_reward']))
           break

DI-zoo Runable Code Example
============================

The complete training configuration file is at `github link <https://github.com/opendilab/DI-engine/tree/main/dizoo/d4rl/config>`__
Inside, for specific configuration files, such as \ ``https://github.com/opendilab/DI-engine/blob/main/dizoo/d4rl/config/hopper_medium_cql_default_config.py``\ , use the following demo to run :

.. code :: python

    from easydict import EasyDict

    from easydict import EasyDict

    hopper_medium_cql_default_config = dict(
        env=dict(
            env_id='hopper-medium-v0',
            norm_obs=dict(use_norm=False, ),
            norm_reward=dict(use_norm=False, ),
            collector_env_num=1,
            evaluator_env_num=8,
            use_act_scale=True,
            n_evaluator_episode=8,
            stop_value=6000,
        ),
        policy=dict(
            cuda=True,
            model=dict(
                obs_shape=11,
                action_shape=3,
                twin_critic=True,
                actor_head_type='reparameterization',
                actor_head_hidden_size=256,
                critic_head_hidden_size=256,
            ),
            learn=dict(
                data_path=None,
                train_epoch=30000,
                batch_size=256,
                learning_rate_q=3e-4,
                learning_rate_policy=1e-4,
                learning_rate_alpha=1e-4,
                ignore_done=False,
                target_theta=0.005,
                discount_factor=0.99,
                alpha=0.2,
                reparameterization=True,
                auto_alpha=False,
                lagrange_thresh=-1.0,
                min_q_weight=5.0,
            ),
            collect=dict(
                n_sample=1,
                unroll_len=1,
                data_type='d4rl',
            ),
            command=dict(),
            eval=dict(evaluator=dict(eval_freq=500, )),
            other=dict(replay_buffer=dict(replay_buffer_size=2000000, ), ),
        ),
    )

    hopper_medium_cql_default_config = EasyDict(hopper_medium_cql_default_config)
    main_config = hopper_medium_cql_default_config

    hopper_medium_cql_default_create_config = dict(
        env=dict(
            type='d4rl',
            import_names=['dizoo.d4rl.envs.d4rl_env'],
        ),
        env_manager=dict(type='base'),
        policy=dict(
            type='cql',
            import_names=['ding.policy.cql'],
        ),
        replay_buffer=dict(type='naive', ),
    )
    hopper_medium_cql_default_create_config = EasyDict(hopper_medium_cql_default_create_config)
    create_config = hopper_medium_cql_default_create_config

Note: For offline RL algorithms, such as TD3_bc, CQL, special entry functions need to be used. For examples, please refer to
`link <https://github.com/opendilab/DI-engine/blob/main/dizoo/d4rl/entry/d4rl_cql_main.py>`__

Benchmark Algorithm Performance
===============

- Walker2d

   - walker2d-medium-expert-v0 + CQL

   .. image :: images/walker2d_medium_expert_cql.png
     : align : center

   - General iteration 1M iteration takes 9 hours (NVIDIA V100)
